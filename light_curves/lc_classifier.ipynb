{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Curve Classifier\n",
    "***\n",
    "\n",
    "## Learning Goals\n",
    "By the end of this tutorial, you will be able to:\n",
    "- do some basic data cleaning and filtering to prepare the data for the ML algorithms\n",
    "- work with Pandas multiindex data frames as a way of storing time domain datasets\n",
    "- use sktime algorithms to train a classifier and predict values on a test dataset\n",
    "\n",
    "## Introduction\n",
    "This notebook takes output of a previous demo notebook which generates light curves from archival data, does data prep, and runs the light curves through multiple [`sktime`](https://www.sktime.net/en/stable/) classifiers.  The goal of the classifiers is to be able to differentiate changing look active galactic nucleii (CLAGN) from an SDSS quasar sample based on multiband light curves.  CLAGN are quite interested objects in that they appear to change state, but only a few hundred are currently known, and finding them is quite expensive requiring spectroscopic follow up.  Being able to identify CLAGN in existing large samples would allow us to identify a statisitcal sample from which we could better understand the physics of what is occuring in these systems.\n",
    "\n",
    "The challenges of this time-domain dataset are:\n",
    "1. Multi-variate = There are multiple bands of observations per target (13+)\n",
    "2. Unequal length = Each band has a light curve with different sampling than other bands\n",
    "3. Missing data = Not each object has all observations in all bands\n",
    "\n",
    "We choose to use a Pandas multiindex dataframe to store and work with the data because it fulfills these requirements:\n",
    "1. It can handle the above challenges of a dataset = multi-variate, unqueal length with missing data.\n",
    "2. Multiple targets (multiple rows)\n",
    "3. Pandas has some built in understanding of time units\n",
    "4. Can be scaled up to big data numbers of rows (altough we don't push to out of memory structures in this use case)\n",
    "5. Pandas is user friendly\n",
    "\n",
    "\n",
    "## Input\n",
    "Light curve parquet file of multiband light curves from the mulitband_lc.ipynb demo notebook.  The format of the light curves is a Pandas multiindex data frame\n",
    "\n",
    "A useful reference for what sktime expects as input to its ML algorithms: https://github.com/sktime/sktime/blob/main/examples/AA_datatypes_and_datasets.ipynb\n",
    "\n",
    "## Output\n",
    "Trained classifiers as well as estimates of their accuracy and plots of confusion matrices\n",
    "\n",
    "## Imports\n",
    "- `pandas` to work with light curve data structure\n",
    "- `numpy` for numerical calculations\n",
    "- `matplotlib` for plotting\n",
    "- `sys` for paths\n",
    "- `astropy` to work with coordinates/units and data structures\n",
    "- `tqdm` for showing progress meter\n",
    "- `sktime` ML algorithms specifically for time-domain data\n",
    "- `sklearn` general use ML algorthims with easy to use interface\n",
    "\n",
    "## Authors\n",
    "\n",
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ensure all dependencies are installed\n",
    "!pip install -r requirements-lc_classifier.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.time import Time\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.classification.deep_learning import CNNClassifier\n",
    "from sktime.classification.dictionary_based import IndividualTDE\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.classification.dummy import DummyClassifier\n",
    "from sktime.classification.ensemble import WeightedEnsembleClassifier\n",
    "from sktime.classification.feature_based import Catch22Classifier, RandomIntervalClassifier\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "from sktime.classification.interval_based import CanonicalIntervalForest\n",
    "from sktime.classification.kernel_based import Arsenal, RocketClassifier\n",
    "from sktime.classification.shapelet_based import ShapeletTransformClassifier\n",
    "from sktime.registry import all_estimators, all_tags\n",
    "from tqdm import tqdm\n",
    "\n",
    "# local code imports\n",
    "sys.path.append('code/')\n",
    "from fluxconversions import mjd_to_jd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in a dataset of archival light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access structure of light curves made in the light curve notebook\n",
    "# has CLAGN & SDSS small sample, all bands\n",
    "#https://drive.google.com/file/d/13RiPODiz2kI8j1OKpP1vfh6ByIUNsKEz/view?usp=share_link\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='13RiPODiz2kI8j1OKpP1vfh6ByIUNsKEz',\n",
    "                                    dest_path='./data/df_lc_458sample.parquet',\n",
    "                                    unzip=True)\n",
    "\n",
    "df_lc = pd.read_parquet(\"./data/df_lc_458sample.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Prep\n",
    "This dataset needs significant work before it can be fed into a ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does the dataset look like anyway?\n",
    "df_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Remove bands with not enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##what are the unique set of bands included in our light curves\n",
    "df_lc.index.unique('band').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get rid of some of the bands that don't have enough data for all the sources\n",
    "df_lc.drop('IceCube', level = 2, axis = 0, inplace = True)\n",
    "df_lc.drop('TESS', level = 2, axis = 0, inplace = True)\n",
    "df_lc.drop('FERMIGTRIG', level = 2, axis = 0, inplace = True)\n",
    "df_lc.drop('K2', level = 2, axis = 0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Combine Labels for a Simpler Classification\n",
    "All CLAGN start in the dataset as having labels based on their discovery paper.  Because we want one sample with all known CLAGN, change those discoery names to be simply \"CLAGN\" for all CLAGN, regardless of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can't change an index in place in a pandas multiindex df\n",
    "#so first need to get rid of the multiindex\n",
    "df_lc = df_lc.reset_index()  \n",
    "\n",
    "df_lc['label'] = df_lc.label.str.replace('MacLeod 16', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('LaMassa 15', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Yang 18', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Lyu 21', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Hon 22', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Sheng 20', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('MacLeod 19', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Green 22', 'CLAGN')\n",
    "df_lc['label'] = df_lc.label.str.replace('Lopez-Navas 22', 'CLAGN')\n",
    "\n",
    "#turn dataframe back into multiindex df\n",
    "df_lc = df_lc.set_index([\"objectid\", \"label\", \"band\", \"time\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove some rows based on filtering out bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and drop bad rows\n",
    "df_lc = df_lc.reset_index()\n",
    "\n",
    "#one panstarrs z has a crazy err value of -999000, definitely don't want to include that one\n",
    "querystring = 'err < -100'\n",
    "df_lc = df_lc.drop(df_lc.query(querystring).index)\n",
    "\n",
    "#drop rows which have Nans\n",
    "df_lc.dropna(inplace = True, axis = 0)\n",
    "\n",
    "df_lc = df_lc.set_index([\"objectid\", \"label\", \"band\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider Removing all rows with errors outside of a normal distribution\n",
    "#plot histograms in flux units to see what these distributions look like\n",
    "\n",
    "band_list = df_lc.index.unique('band').tolist()\n",
    "threesigmaonmean= []\n",
    "\n",
    "#create the figure and axes\n",
    "fig, axs = plt.subplots(5, 3, figsize = (12, 12))\n",
    "\n",
    "# unpack all the axes subplots\n",
    "axe = axs.ravel()\n",
    "\n",
    "#for each band\n",
    "for count, bandname in enumerate(band_list):\n",
    "    singleband = df_lc.loc[:,:,bandname,:]\n",
    "    #what are the standard deviations of these distributions\n",
    "    cut = singleband['err'].mean() + 3 * (singleband['err'].std())\n",
    "    threesigmaonmean.append(cut)\n",
    "    #plot distributions and print stddev\n",
    "    singleband['err'].plot(kind = 'hist', bins = 30, subplots =True, ax = axe[count],label = bandname+' '+str(cut), legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3sigma looks by eye like a reasonable cut for all distributions\n",
    "\n",
    "#to make the cut, need to first remove the index\n",
    "df_lc = df_lc.reset_index()\n",
    "\n",
    "#create the list of which rows to cut and drop them from the dataframe\n",
    "for count, bandname in enumerate(band_list):\n",
    "    querystring = f'band == {bandname!r} & err > {threesigmaonmean[count]}'\n",
    "    print(querystring)\n",
    "    df_lc = df_lc.drop(df_lc.query(querystring).index)\n",
    "\n",
    "#reset the index\n",
    "df_lc = df_lc.set_index([\"objectid\", \"label\", \"band\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this removed ~2% of the rows, which is reasonable and we haven't done anything drastic\n",
    "df_lc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Make a new time column with datetime data type \n",
    "We need the times in python [datetime](https://docs.python.org/3/library/datetime.html) format for the next section where we will make the light curves be of uniform length. \n",
    "\n",
    "Once the time column is in datetime format, make it the time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to convert df_lc time into datetime\n",
    "#start by getting rid of index so that we can work with the columns\n",
    "mjd = df_lc.reset_index().time\n",
    "\n",
    "#convert to JD\n",
    "jd = mjd_to_jd(mjd)\n",
    "\n",
    "#convert to individual components\n",
    "t = Time(jd, format = 'jd' )\n",
    "\n",
    "#t.datetime is now an array of type datetime\n",
    "#make it a column in the dataframe\n",
    "df_lc['datetime'] = t.datetime\n",
    "\n",
    "#remove the time index and make datetime the index\n",
    "df_lc = df_lc.droplevel('time')\n",
    "\n",
    "#set the multiindex back in place\n",
    "df_lc.set_index('datetime', append = True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Make all objects and bands have identical time arrays (uniform length and spacing)\n",
    "\n",
    "It is very hard to find time-domain ML algorithms which can handle non uniform length datasets. Therefore we make them uniform using Pandas [reindex](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html) which fills in the uniform length arrays with values according to the method chosen by the user.  We implement a nearest neighbor to fill the arrays.  \n",
    "\n",
    "Fluxes with missing data are set to zero.  Zero was chosen instead of 'None' or 'Nan' as it was better integrated with the ML algorithms.  \n",
    "\n",
    "Potential other options for uniformizing the time series dataset:\n",
    "- pandas.dataframe.interpolate with many methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#This cell takes about ~10 min to run depending on your server\n",
    "\n",
    "#make a new index with the full range of datetimes and a specified frequency\n",
    "final_freq = '30D'\n",
    "fullindex = pd.date_range(start=df_lc.droplevel('objectid').droplevel('label').droplevel('band').index.min(), end=df_lc.droplevel('objectid').droplevel('label').droplevel('band').index.max(), freq=final_freq)\n",
    "\n",
    "#what is the full set of unique band names?\n",
    "full_bandname = df_lc.index.unique('band').tolist()\n",
    "\n",
    "#setup to hold the resulting dataframe\n",
    "df_reindex = pd.DataFrame()\n",
    "\n",
    "#how many objects are in the dataframe?\n",
    "#actually this is the max and some of them are missing\n",
    "nobj = df_lc.index.unique('objectid').max() + 1\n",
    "\n",
    "#for each objct in the dataframe\n",
    "for oid in tqdm(range(nobj)):\n",
    "    try:\n",
    "        oid_bandname = df_lc.loc[oid,:,:,:].index.unique('band').tolist()\n",
    "    except KeyError:\n",
    "        #some of the data cleaning above must have removed some objects\n",
    "        print(oid, ' objectid doesnt exist')\n",
    "    else:\n",
    "        #this is the case where the objectid exists, so let's make sure all the bands are there\n",
    "        oid_label = df_lc.loc[oid,:,:,:].index.unique('label').tolist()\n",
    "    \n",
    "        #if bandname for this oid is not equal to the full list\n",
    "        #If there are missing bands for this object\n",
    "        if oid_bandname != full_bandname:\n",
    "    \n",
    "            #figure out which bands are missing\n",
    "            missing = list(set(full_bandname).difference(oid_bandname))\n",
    "        \n",
    "            #make new dataframe for this object with NaN flux and err values\n",
    "            #choosing random times for now that will get fleshed out when I reindex all times below\n",
    "            for m in range(len(missing)):\n",
    "                datetimelist = [pd.to_datetime(1490195805, unit='s'), pd.to_datetime(1491195805, unit='s')]  #making this up for now\n",
    "                dfsingle = pd.DataFrame(dict(flux=np.zeros(2), err=np.zeros(2), datetime=datetimelist, objectid=oid, band=missing[m],label=oid_label[0])).set_index([\"objectid\",\"label\", \"band\", \"datetime\"])        \n",
    "\n",
    "                #now concatenate the single new df with df_lc\n",
    "                df_lc = pd.concat([df_lc, dfsingle])\n",
    "            \n",
    "        #for each band\n",
    "        for bandid in range(len(full_bandname)):\n",
    "    \n",
    "            #set up to only have time as an index\n",
    "            single_obj = df_lc.loc[oid,:,full_bandname[bandid],:].reset_index().set_index('datetime')\n",
    "    \n",
    "            #sort the time index\n",
    "            single_obj = single_obj.sort_index()\n",
    "    \n",
    "            #change the time index to have values in fullindex, and use 'nearest' to fill\n",
    "            #in values of the new time indices\n",
    "            #this is where the real work gets done\n",
    "            single_reindex = single_obj.reindex(fullindex, method = 'nearest')\n",
    "    \n",
    "            #previous command returns an unnamed index, so name it\n",
    "            single_reindex.index.rename('datetime', inplace= True)\n",
    "    \n",
    "            #then we really want a multiindex df, so first remove indices, then set the ones we want\n",
    "            single_reindex.reset_index(inplace= True)\n",
    "            \n",
    "            #put oid and bandname back into the dataframe\n",
    "            single_reindex['objectid'] = oid\n",
    "            single_reindex['band'] = full_bandname[bandid]\n",
    "            \n",
    "            #re-make index\n",
    "            single_reindex.set_index([\"objectid\" ,\"label\",\"band\", \"datetime\"], inplace = True)\n",
    "    \n",
    "            #concatenate all the dfs together\n",
    "            if df_reindex.empty: \n",
    "                df_reindex = single_reindex\n",
    "            else:\n",
    "                df_reindex = pd.concat([df_reindex, single_reindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#proove this did what I wanted it to do\n",
    "oid = 0\n",
    "#how many bandnames in object 0?\n",
    "bn = df_lc.loc[oid,:,:,:].index.unique('band').tolist()\n",
    "print('there are', bn, 'bandnames ')\n",
    "#how many times per band in object 0\n",
    "oid_bandname = df_reindex.loc[oid,:,:,:].index.unique('band').tolist()\n",
    "for i in range(len(oid_bandname) ):\n",
    "    times = df_reindex.loc[oid,:,oid_bandname[i],:].index.unique('datetime').tolist()\n",
    "    print('for band', oid_bandname[i],' there are ', len(times),' times')\n",
    "    \n",
    "    \n",
    "#output from this cell should show that all bands have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out if this 'nearest' reindexing is a reasonable choice for this use case\n",
    "#plotting the light curves before and after reindexing.\n",
    "\n",
    "#pick one object, one band for now\n",
    "dfsingle_before = df_lc.loc[4,:,'panstarrs r',:]\n",
    "dfsingle_after = df_reindex.loc[4,:,'panstarrs r',:]\n",
    "\n",
    "#time in a format matplotlib can handle\n",
    "t1 = dfsingle_before.index.get_level_values('datetime').to_pydatetime()\n",
    "f1 = dfsingle_before.flux\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "f = ax.plot(t1, f1, label = \"original\") \n",
    "f = ax.scatter(t1, f1, label = \"original\")\n",
    "\n",
    "t2 = dfsingle_after.index.get_level_values('datetime').to_pydatetime()\n",
    "f2 = dfsingle_after.flux\n",
    "f = ax.plot(t2, f2, label = \"reindexed\") \n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Normalize to the mean of each objectid\n",
    "- this is normalizing across all bands\n",
    "- think this is the right place to do this, rather than before we reindex over time \n",
    "    so that the final light curves are normalized since that is the chunk of information \n",
    "    which goes into the ML algorithms.\n",
    "- chose mean and not median because there are some objects where the median flux = 0.0\n",
    "    - if we did this before the reindexing, the median might be a non-zero value\n",
    "- normalizing is required so that the CLAGN and it's comparison SDSS sample don't have different flux levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does the dataframe currently look like\n",
    "df_reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea here is that we normalize across each object.  \n",
    "#so the code will know for example that within one object W1 will be brighter than ZTF bands\n",
    "#but from one object to the next, it will not know that one is brighter than the other."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#want to normalize the light curves across all bands\n",
    "#this cell is intentionally made 'raw' format while the normalization part of this code is a work in progress. \n",
    "\n",
    "#setup new column to hold the normalized fluxes\n",
    "df_reindex[[\"normflux\"]] = 100\n",
    "\n",
    "#can't figure out how to do this when using multiindex, so get rid of it.\n",
    "#remove the index\n",
    "df_reindex_reset = df_reindex.reset_index()\n",
    "\n",
    "\n",
    "#how many objects are in the dataframe?\n",
    "#actually this is the max and some of them are missing\n",
    "nobj = df_reindex.index.unique('objectid').max() + 1\n",
    "\n",
    "\n",
    "# get an iterable of all objectids in the dataframe\n",
    "objectids = parallel_df_lc.data.index.unique(\"objectid\")\n",
    "\n",
    "#for each objct in the dataframe\n",
    "\n",
    "for oid in objectids:\n",
    "    #what is the median flux of all bands for this object?\n",
    "    meanflux = df_reindex.loc[oid,:,:,:].flux.mean(skipna = True)\n",
    "    print('mean flux', meanflux)\n",
    "    \n",
    "    querystring = f'objectid == {oid}'\n",
    "    normflux = df_reindex_reset.query(querystring).flux.values / meanflux\n",
    "    datetime = df_reindex_reset.query(querystring).datetime.values\n",
    "    objectid = df_reindex_reset.query(querystring).objectid.values\n",
    "    band = df_reindex_reset.query(querystring).band.values\n",
    "    label = df_reindex_reset.query(querystring).label.values\n",
    "    dfsingle = pd.DataFrame(dict(flux=normflux, err=df_reindex.loc[oid,:,:,:].err, datetime=datetime, objectid=objectid, band=band,label=label))\n",
    "\n",
    "    #now concatenate the single new df with df_lc\n",
    "    if oid == 0:\n",
    "        df_norm = dfsingle\n",
    "    else:\n",
    "        df_norm = pd.concat([df_norm, dfsingle])\n",
    "            \n",
    "            \n",
    "#reset the index\n",
    "df_norm = df_norm.set_index([\"objectid\", \"label\", \"band\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try standardizing each object separately\n",
    "#same idea that we are trying to remove the signal that some objects are brighter than others\n",
    "#but keep the info about relative differences within each object.\n",
    "\n",
    "#setup new column to hold the normalized fluxes\n",
    "df_reindex[[\"normflux\"]] = 100\n",
    "\n",
    "#can't figure out how to do this when using multiindex, so get rid of it for a minute\n",
    "#remove the index\n",
    "df_reindex_reset = df_reindex.reset_index()\n",
    "\n",
    "\n",
    "#how many objects are in the dataframe?\n",
    "#actually this is the max and some of them are missing\n",
    "nobj = df_reindex.index.unique('objectid').max() + 1\n",
    "\n",
    "#for each objct in the dataframe\n",
    "for oid in tqdm(range(nobj)):\n",
    "    try:\n",
    "        #what is the median flux of all bands for this object?\n",
    "        meanflux = df_reindex.loc[oid,:,:,:].flux.mean(skipna = True)\n",
    "        stdflux = df_reindex.loc[oid,:,:,:].flux.std(skipna=True)\n",
    "        print('mean flux', meanflux)\n",
    "    except KeyError:\n",
    "        #some of the data cleaning above must have removed some objects\n",
    "        print(oid, ' objectid doesnt exist')\n",
    "    else:\n",
    "        querystring = f'objectid == {oid}'\n",
    "        zflux = (df_reindex_reset.query(querystring).flux.values - meanflux) / stdflux\n",
    "        datetime = df_reindex_reset.query(querystring).datetime.values\n",
    "        objectid = df_reindex_reset.query(querystring).objectid.values\n",
    "        band = df_reindex_reset.query(querystring).band.values\n",
    "        label = df_reindex_reset.query(querystring).label.values\n",
    "        dfsingle = pd.DataFrame(dict(flux=zflux, err=df_reindex.loc[oid,:,:,:].err, datetime=datetime, objectid=objectid, band=band,label=label))\n",
    "\n",
    "        #now concatenate the single new df with df_lc\n",
    "        if oid == 0:\n",
    "            df_norm = dfsingle\n",
    "        else:\n",
    "            df_norm = pd.concat([df_norm, dfsingle])\n",
    "            \n",
    "            \n",
    "#reset the index\n",
    "df_norm = df_norm.set_index([\"objectid\", \"label\", \"band\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proove this did what I wanted it to do for one randomly chosen object\n",
    "oid = 318\n",
    "meanflux = df_norm.loc[oid,:,:,:].flux.mean(skipna = True)\n",
    "print(meanflux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Restructure dataframe in format expected by sktime\n",
    "- Make columns have band names in them and then remove band from the index\n",
    "- pivot the dataframe so that SKTIME understands its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df_reindex = df_reindex.reset_index(level=\"band\").pivot(columns=\"band\")\n",
    "pivoted_df_reindex.columns = [\"_\".join(col) for col in pivoted_df_reindex.columns.values]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#proove this did what I wanted it to do\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(pivoted_df_reindex.loc[4,:,:,:])\n",
    "pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df_reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this dataframe to use for the ML below so we don't have to make it every time\n",
    "parquet_savename = 'output/df_ML_091523.parquet'\n",
    "pivoted_df_reindex.to_parquet(parquet_savename)\n",
    "#print(\"file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could load a previously saved file in order to plot\n",
    "#parquet_loadname = 'output/df_ML_091523.parquet'\n",
    "#pivoted_df_reindex = MultiIndexDFObject()\n",
    "#pivoted_df_reindex.data = pd.read_parquet(parquet_loadname)\n",
    "#print(\"file loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prep for ML algorithms in sktime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Consider data augmentation\n",
    "\n",
    "1. https://arxiv.org/pdf/1811.08295.pdf which has the following github\n",
    "\n",
    "    - https://github.com/gioramponi/GAN_Time_Series/tree/master\n",
    "    - not easily usable\n",
    "2. https://arxiv.org/pdf/2205.06758.pdf\n",
    "\n",
    "3. ChatGPT - give multiindex df function and it will give a starting point for augmenting\n",
    "\n",
    "\n",
    "Worried that augmenting noisy data just makes more noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train test split \n",
    "- Because thre are uneven numbers of each type (many more SDSS than CLAGN), we want to make sure to stratify evenly by type\n",
    "- Random split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y is defined to be the labels\n",
    "y = pivoted_df_reindex.droplevel('datetime').index.unique().get_level_values('label').to_series()\n",
    "\n",
    "#want a stratified split based on label\n",
    "train_ix, test_ix = train_test_split(pivoted_df_reindex.index.levels[0], stratify = y, shuffle = True, random_state = 43, test_size = 0.25)\n",
    "\n",
    "train_df = pivoted_df_reindex.loc[train_ix]  \n",
    "test_df = pivoted_df_reindex.loc[test_ix]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does it look like?\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to show how many of each type of object in the test dataset\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Objects in the Test dataset\")\n",
    "h = plt.hist(test_df.droplevel('datetime').index.unique().get_level_values('label').to_series(),histtype='stepfilled',orientation='horizontal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convert df_lc into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the dataframe into X and y for ML algorithms \n",
    "\n",
    "#X is the multiindex light curve without the labels\n",
    "X_train  = train_df.droplevel('label')\n",
    "X_test = test_df.droplevel('label')\n",
    "\n",
    "#y are the labels, should be a series \n",
    "y_train = train_df.droplevel('datetime').index.unique().get_level_values('label').to_series()\n",
    "y_test = test_df.droplevel('datetime').index.unique().get_level_values('label').to_series()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check that the data types are ok for sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask sktime if it likes the data type of X\n",
    "from sktime.datatypes import check_is_mtype\n",
    "\n",
    "check_is_mtype(X_train, mtype=\"pd-multiindex\", scitype=\"Panel\", return_metadata=True)\n",
    "#check_is_mtype(X_test, mtype=\"pd-multiindex\", scitype=\"Panel\", return_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Machine Learning Algorithms on the light curves\n",
    "\n",
    "We choose to use [sktime](https://www.sktime.net/en/stable/index.html) algorithms beacuse it is a library of many algorithms specifically tailored to time series datasets.  It is based on the sklearn library so syntax is familiar to many users.\n",
    "\n",
    "Types of classifiers are listed [here](https://www.sktime.net/en/stable/api_reference/classification.html).\n",
    "\n",
    "This notebook will invert the actual workflow and show you a single example of the algorithm which best fits the data and has the most accurate classifier. Then it will show how to write a for loop over a bunch of classifiers before narrowing it down to the most accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#what is the list of all possible classifiers that work with multivariate data\n",
    "#all_tags(estimator_types = 'classifier')\n",
    "classifiers = all_estimators(\"classifier\", filter_tags={'capability:multivariate':True})\n",
    "classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The Most Accurate Classifier\n",
    "See section 4.2 for how we landed with this algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like RandomIntervalClassifier is performing the best for the CLAGN (not for the SDSS)\n",
    "\n",
    "#setup the classifier\n",
    "clf = RandomIntervalClassifier(n_intervals = 20, n_jobs = -1, random_state = 43)\n",
    "\n",
    "#fit the classifier on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#make predictions on the test dataset using the trained model \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy of Random Interval Classifier: {accuracy_score(y_test, y_pred)}\\n\", flush=True)\n",
    "\n",
    "#plot a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loop over a bunch of classifiers\n",
    "\n",
    "Our method is to do a cursory check of a bunch of classifiers and then later drill down deeper on anything with good initial results.  We choose to run a loop over ~10 classifiers that seem promising and check the accuracy scores for each one.  Any classifier with a promising accuracy score could then be followed up with detailed hyperparameter tuning, or potentially with considering other classifiers in that same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which classifiers are we interestd in\n",
    "#roughly one from each type of classifier\n",
    "\n",
    "names = [\"Arsenal\",                     #kernel based\n",
    "        \"RocektClassifier\",             #kernel based\n",
    "        \"CanonicalIntervalForest\",      #interval based\n",
    "        \"HIVECOTEV2\",                   #hybrid\n",
    "#        \"CNNClassifier\",               #Deep Learning  - **requires tensorflow which is giving import errors\n",
    "#        \"WeightedEnsembleClassifier\",   #Ensemble - **maybe use in the future if we find good options\n",
    "        \"IndividualTDE\",               #Dictionary-based\n",
    "        \"KNeighborsTimeSeriesClassifier\", #Distance Based\n",
    "        \"RandomIntervalClassifier\",     #Feature based\n",
    "        \"Catch22Classifier\",            #Feature based\n",
    "        \"ShapeletTransformClassifier\"   #Shapelet based\n",
    "        \"DummyClassifier\"]             #Dummy - ignores input\n",
    "\n",
    "#for those with an impossible time limit, how long to let them run for before cutting off\n",
    "nmins = 10\n",
    "\n",
    "#these could certainly be more tailored\n",
    "classifier_call = [Arsenal(time_limit_in_minutes=nmins, n_jobs = -1), \n",
    "                  RocketClassifier(num_kernels=2000),\n",
    "                  CanonicalIntervalForest(n_jobs = -1),\n",
    "                  HIVECOTEV2(time_limit_in_minutes=nmins, n_jobs = -1),\n",
    "#                  CNNClassifier(),\n",
    "#                  WeightedEnsembleClassifier(),\n",
    "                  IndividualTDE(n_jobs=-1),\n",
    "                  KNeighborsTimeSeriesClassifier(n_jobs = -1),\n",
    "                  RandomIntervalClassifier(n_intervals = 20, n_jobs = -1, random_state = 43),\n",
    "                  Catch22Classifier(outlier_norm = True, n_jobs = -1, random_state = 43),\n",
    "                  ShapeletTransformClassifier(time_limit_in_minutes=nmins,n_jobs = -1),\n",
    "                  DummyClassifier()]\n",
    "\n",
    "#setup to store the accuracy scores\n",
    "accscore_dict = {}\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in tqdm(zip(names, classifier_call)):\n",
    "    #fit the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #make predictions on the test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    #calculate and track accuracy score\n",
    "    accscore = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of {name} classifier: {accscore}\\n\", flush=True)\n",
    "    accscore_dict[name] = accscore\n",
    "    \n",
    "    #plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "#just for keeping track, I also tried \n",
    "#clf = SignatureClassifier(depth = 2, window_depth = 3, random_state = 43)\n",
    "#this fails to complete, and is a known limitation of this algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the summary of the algorithms used and their accuracy score\n",
    "accscore_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Conclusions:  \n",
    "This classifier can be used to predict CLAGN.  The feature based algorithms do the best jobs of having little to no predicted CLAGN that are truly normal SDSS quasars.  We infer then that if the trained model predicts CLAGN, it is a very good target for follow-up spectroscopy to confirm CLAGN.  However this algorthim will not catch all CLAGN, and will incorrectly labels some CLAGN as being normal SDSS quasars.  THis algorithm can therefore not be used to find a complete sample of CLAGN, but can be used to increase the known sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Potential Areas of improvement\n",
    "- Data is messy\n",
    "    - ZTF calibration??\n",
    "- Label inaccuracy is a concern\n",
    "    - mostly SDSS, \n",
    "    - but CLAGN papers all have different selection criteria\n",
    "- Not enough data on CLAGN\n",
    "    - limited number of lightcurves\n",
    "    - consider data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
